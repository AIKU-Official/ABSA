
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "                                                                           | 0/4693 [00:02<?, ?it/s]







  warnings.warn("To get the last learning rate computed by the scheduler, "                                                                           | 49/4693 [00:19<27:54,  2.77it/s]


























































































































































































































































































































































































































































































































































































































































































































































































































You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.693/4693 [26:59<00:00,  3.18it/s]
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'.
The class this function is called from is 'BartTokenizer'.
                  Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
0it [00:00, ?it/s]



























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Traceback (most recent call last):
  File "C:\Users\bestm\Desktop\Dev\AIKU\ABSA\ACSA-generation\train_MAMS_sentiment.py", line 62, in <module>
    best_accuracy = model.train_model(train_df, best_accuracy)
  File "C:\Users\bestm\Desktop\Dev\AIKU\ABSA\ACSA-generation\seq2seq_model_M.py", line 283, in train_model
    global_step, tr_loss, best_accuracy = self.train(
  File "C:\Users\bestm\Desktop\Dev\AIKU\ABSA\ACSA-generation\seq2seq_model_M.py", line 621, in train
    accuracy = predict_test(model, self.device)
  File "C:\Users\bestm\Desktop\Dev\AIKU\ABSA\ACSA-generation\test_MAMS.py", line 130, in predict_test
    json.dump(out_json, f, indent=4, ensure_ascii=False)
  File "C:\ProgramData\Anaconda3\envs\pytorch\lib\json\__init__.py", line 180, in dump
    fp.write(chunk)
UnicodeEncodeError: 'cp949' codec can't encode character '\u2714' in position 1: illegal multibyte sequence